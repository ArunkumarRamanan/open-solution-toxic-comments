project-key: TOX

name: toxic
tags: [char, vdcnn, adam]


metric:
  channel: 'Final Validation Score ROC_AUC'
  goal: maximize

#Comment out if not in Cloud Environment
#pip-requirements-file: requirements.txt

exclude:
  - output
  - imgs
  - neptune.log
  - offline_job.log
  - .git
  - .idea
  - .ipynb_checkpoints

parameters:
  # Cloud Environment
#  data_dir: /public/toxic_comments
#  experiment_dir: /output/trained_pipelines/glove_lstm
#  embedding_filepath: /public/models/glove/glove.840B.300d.txt
  # Local Environment
  data_dir: /mnt/ml-team/minerva/toxic/data
  single_model_predictions_dir: /mnt/ml-team/minerva/toxic/single_model_predictions_newest
  experiment_dir: /mnt/ml-team/minerva/toxic/trained_pipelines/char_vdcnn
  embedding_filepath: /mnt/ml-team/minerva/pretrained/fasttext/crawl-300d-2M.vec
  bad_words_filepath: external_data/compiled_bad_words.txt
  overwrite: 0
  num_workers: -2
  n_cv_splits: 5

  # Preprocessing
  max_features_char: 100 #100 for vdcnn, for tfidf something like 20000 should be used
  max_features_word: 100000
  maxlen_char: 1000
  maxlen_words: 200
  char_ngram_max: 4
  drop_punctuation: 0
  drop_newline: 0
  drop_multispaces: 0
  all_lower_case: 0
  fill_na_with: ' '

  # Architecture
  filter_nr:
    value: [16,32,64]
  kernel_size: 3
  repeat_block:
    value: [2,4,6]
  dense_size: 256
  repeat_dense:
    value: [1,2]
  max_pooling: 1
  mean_pooling:
    value: [0,1]
  weighted_average_attention: 0
  concat_mode: 'concat'
  trainable_embedding: 1
  word_embedding_size: None
  char_embedding_size:
    value: [16,32]

  # General Architecture
  use_prelu: 1

 # Log Reg Params
  log_reg_c: 100
  log_reg_penalty: 'l2'
  max_iter: 1000

  # Ensemble Catboost
  catboost__iterations: 500
  catboost__learning_rate: 0.02
  catboost__depth: 6
  catboost__l2_leaf_reg: 10
  catboost__border_count: 100
  catboost__model_size_reg: None # Todo: Explore
  catboost__rsm: None # Todo: Explore
  catboost__verbose: 0

  # Training schedule
  epochs_nr: 1000
  batch_size_train: 128
  batch_size_inference: 128
  lr: 0.005
  momentum: 0.9
  gamma: 0.8
  patience: 10

  # Regularization
  batch_norm_first: 1
  use_batch_norm: 1
  dropout_embedding: 0.0
  rnn_dropout: 0.5
  dense_dropout: 0.0
  conv_dropout: 0.0
  dropout_mode: 'spatial'
  rnn_kernel_reg_l2: 0.0000
  rnn_recurrent_reg_l2: 0.000
  rnn_bias_reg_l2: 0.000
  dense_kernel_reg_l2: 0.0000
  dense_bias_reg_l2: 0.0000
  conv_kernel_reg_l2: 0.0000
  conv_bias_reg_l2: 0.0000